{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeig+xIh8xt7nPvooUDVUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyCrypto2019/LLM/blob/main/PyTorchTextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IyZ3K-9Pw20V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import torch\n",
        "import collections\n",
        "from collections import Counter\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from bs4 import BeautifulSoup\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/sample_data//Reviews.csv', nrows = 10000)\n",
        "data = df[['Text', 'Summary']]\n",
        "data.drop_duplicates(subset=['Text'], inplace=True) # Drop duplicate rows"
      ],
      "metadata": {
        "id": "AfgV3bY0xrPU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is the time to do some initial data pre-processing which involves cleaning data, removing duplicates and deleting rows which has the summary column as blank. First of all we will use a word mapping which will map phrases like “I’ve” to “I have” and so on. A dictionary is declared with the key value and the text and summary fields are processed."
      ],
      "metadata": {
        "id": "NvFWOD0D-cKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_mapping = {\"ain't\": \"is not\",\"aint\": \"is not\", \"aren't\": \"are not\",\"arent\": \"are not\",\"can't\": \"cannot\",\"cant\": \"cannot\", \"'cause\": \"because\", \"cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", 'mstake':\"mistake\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\",'wasnt':\"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\", 'youve':\"you have\", 'goin':\"going\", '4ward':\"forward\", \"shant\":\"shall not\",'tat':\"that\", 'u':\"you\", 'v': \"we\",'b4':'before', \"sayin'\":\"saying\"\n",
        "                      }"
      ],
      "metadata": {
        "id": "MffRoLdQ_QP7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define a function that converts the input text into lower case, removes any text within parenthesis along with the parenthesis (since you will not mention something important using brackets), remove the quotation marks, any word which does not start with an alphabet. The words with word length less than 3 is also removed. Finally a space is added to all the remaining punctuations because otherwise the words ‘my’ and ‘my,’ will be treated as two different words when they are essentially the same. Both the text and the summary data are pre-processed."
      ],
      "metadata": {
        "id": "68q7xc_7_axz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([word_mapping[t] if t in word_mapping else t for t in newString.split(\" \")])\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    long_words=[]\n",
        "\n",
        "    tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:                  #removing short word\n",
        "            long_words.append(i)\n",
        "    text = \" \".join(long_words).strip()\n",
        "    def no_space(word, prev_word):\n",
        "        return word in set(',!\"\";.''?') and prev_word!=\" \"\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
        "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
        "    text = ''.join(out)\n",
        "    return text\n",
        "\n",
        "data['cleaned_text'] = data['Text'].apply(text_cleaner)\n",
        "data['cleaned_summary'] = data['Summary'].apply(text_cleaner)\n",
        "# this step is to remove all rows that have a blank summary\n",
        "data[\"cleaned_summary\"].replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0, inplace=True)\n",
        "#\n",
        "max_len_text=100\n",
        "max_len_summary=10\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv87Aq41_drY",
        "outputId": "38bbc1ec-c037-40b9-fe82-2df6b8bc7608"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we split the data into train and validation sets. This can be statically done with the help of simple python slicing, but I wanted to shuffle the data so have used the train test split function of the sci-kit learn library."
      ],
      "metadata": {
        "id": "-SKCA_AJJv6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = tts(data['cleaned_text'],data['cleaned_summary'],test_size=0.1, shuffle=True, random_state=111)"
      ],
      "metadata": {
        "id": "EpDl1yqjJySp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we tokenize and then pad or truncate the tokens into a fixed length sequence as decided upon before. Since Pytorch is used and for this I will not use torch text, therefore we need to create the functions and the vocabulary class which is done below. In one of my previous posts in Medium I have explained in detail about how to do that. I would request the reader to go through this post for a detailed explanation. Since this is sequence to sequence learning, it is important to append the start of sequence, end of sequence token to each example text and summary for processing. Also if the maximum text length is greater than the text length for a particular example, padding tokens are to be appended. These needs to be exclusively mentioned as special tokens while creating the vocabulary."
      ],
      "metadata": {
        "id": "OB9z-wFSJ8t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize function\n",
        "def tokenize(lines, token='word'):\n",
        "    assert token in ('word', 'char'), 'Unknown token type: ' + token\n",
        "    return [line.split() if token == 'word' else list(line) for line in lines]\n",
        "\n",
        "# pading function\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps]  # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
        "\n",
        "# the vocabulary class\n",
        "class Vocab:\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['<unk>']\n",
        "# tokenize\n",
        "src_tokens = tokenize(x_train)\n",
        "tgt_tokens = tokenize(y_train)\n",
        "# build vocabulary on dataset\n",
        "src_vocab = Vocab(src_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
        "tgt_vocab = Vocab(tgt_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])"
      ],
      "metadata": {
        "id": "b7VSUryIJ--l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create the minibatches of data. For this project I have chosen a batch size of 64. But before that, we have a few more steps. First, we have to pad or truncate the text and the summary to the maximum text and summary sequence length as previously determined. As of now each record of text and sequence is appended an end-of-sequence(eos) token and then padded. The length of the each record along with the eos token is determined for each token and is stored in a vector the length of which is equal to the batch size. The transformed text array, summary array along with the valid length for both the sequences are stored in a tuple as the final data array. Here the text related matrices is addressed as source and the summary related matrices are addressed as target."
      ],
      "metadata": {
        "id": "X_VI9mI9KKQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fn to add eos and padding and also determine valid length of each data sample\n",
        "def build_array_sum(lines, vocab, num_steps):\n",
        "    lines = [vocab[l] for l in lines]\n",
        "    lines = [l + [vocab['<eos>']] for l in lines]\n",
        "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
        "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
        "    return array, valid_len\n",
        "\n",
        "src_array, src_valid_len = build_array_sum(src_tokens, src_vocab, max_len_text)\n",
        "tgt_array, tgt_valid_len = build_array_sum(tgt_tokens, tgt_vocab, max_len_summary)\n",
        "data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)"
      ],
      "metadata": {
        "id": "igeQMGIGKNpw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fetching the data for training the neural network in Pytorch, we need to create the dataset object by passing the data array from the above set by instantiating the TensorDataset class. Next, this object along with the batch size is passed into the DataLoader class of the Pytorch which creates minibatch instances of the training dataset."
      ],
      "metadata": {
        "id": "EZA2Qp8rKgTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tensor dataset object\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "batch_size = 64\n",
        "data_iter = load_array(data_arrays, batch_size)"
      ],
      "metadata": {
        "id": "PXzUwgd-KuBR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us start building the transformer model which is in fact the building block of sophisticated models in NLP(Natural Language Processing) and Machine Learning like BERT, GPT and more. There are two parts to the transformer, an encoder and a decoder. An encoder is made up of multiple similar blocks where there are two main sublayers, the self-attention pooling layer and a position wise feed forward network. Since this is a deep architecture, ResNet design has been employed here, where residual connections are added to both sub layers. Resnet is basically a collection of nested function classes, where the larger function classes contain the smaller ones that increases the expressive power of the network by training them to the identity function f(x) = x. In fact each additional nested layer should more easily contain the identity function as one of it’s elements. Resnet addresses the vanishing gradient problem and also allows shortcut connections within the network, that addresses information loss, since every layer linearly transforms the input which results in some distortion of information. In ResNet we basically try to adjust the weights so that (f(x) — x) gets closer to 0 (i.e. we try to calculate the residual, hence the name). In a particular encoder block, the queries, keys and values are from the previous block. For the first block, the original input tokens are embedded, then positional encodings are added to each token and fed into the first block for processing. The position wise feed forward network transforms the representation at all sequence positions using the same Multi layered perceptron (MLP) and hence using the same weight parameters. A ReLU activation employed here adds some non-linearity and helps in modeling complex relationships between tokens.\n",
        "\n",
        "Transformer Architecture as depicted in the original 2017 paper “Attention is All You Need” by Vaswani et al.\n",
        "After the residual connection is added, layer normalization has been done. This is done because the magnitude of parameters in one layer can widely differ from the other layer. Therefore, the difference in magnitude of outputs of the connected layers will also be varied, therefore normalization along each row is done to adjust that .The flowchart of how the encoder works is shown in the diagram below.\n",
        "Network Flowchart of Encoder part of the Transformer Model\n",
        "Work flow diagram of the encoder part of the transformer\n",
        "The self-attention class using Multi-head attention is shown here.\n"
      ],
      "metadata": {
        "id": "Zgty0xFnKzA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The main class\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = DotProductAttention(dropout)\n",
        "        self.w_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
        "        self.w_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
        "        self.w_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
        "        self.w_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries = transpose_qkv(self.w_q(queries), self.num_heads)\n",
        "        keys = transpose_qkv(self.w_k(keys), self.num_heads)\n",
        "        values = transpose_qkv(self.w_v(values), self.num_heads)\n",
        "        if valid_lens is not None:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        output_concat = transpose_output(output, self.num_heads)\n",
        "        return self.w_o(output_concat)\n",
        "\n",
        "# Function to transpose the linearly transformed query key and values\n",
        "def transpose_qkv(X, num_heads):\n",
        "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "# For output formatting\n",
        "def transpose_output(X, num_heads):\n",
        "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
        "\n",
        "# The dot product attention scoring function\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2))/math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
        "# Here masking is used so that irrelevant padding tokens are not considered\n",
        "# while calculations\n",
        "\n",
        "def sequence_mask(X, valid_len, value=0):\n",
        "    maxlen = X.size(1)\n",
        "    mask = torch.arange((maxlen), dtype=torch.float32)[None, :] < valid_len[:, None]    #device=X.device\n",
        "    X[~mask] = value\n",
        "    return X\n",
        "# the irrelevant tokens are given a very small negative value which gets\n",
        "# ignored in the subsequent calculations\n",
        "def masked_softmax(X, valid_lens):\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ],
      "metadata": {
        "id": "7N1oeGdIK_b-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The position-wise feed forward network code is as shown below.\n"
      ],
      "metadata": {
        "id": "rcpfKYyAA2WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_output, **kwargs):\n",
        "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
        "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_output)\n",
        "    def forward(self, X):\n",
        "        return self.dense2(self.relu(self.dense1(X)))"
      ],
      "metadata": {
        "id": "XGdZlO7wLR3I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encoding is used in both encoder and decoder to maintain the position information of both the source and target tokens. The following code block defines that class. A positional embedding matrix P is used which has the same dimension as the input matrix i.e. the token matrix and X+P is computed. The rows in P matrix corresponds to position in a sequence and columns represent different positional encoding dimensions. The encoding dimensions are coded as follows :"
      ],
      "metadata": {
        "id": "8IYqsJwCx0lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
        "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)/torch.pow(10000,torch.arange(0, num_hiddens,2, dtype=torch.float32)/num_hiddens)\n",
        "        self.P[:,:, 0::2] = torch.sin(X)\n",
        "        self.P[:, :, 1::2] = torch.cos(X)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
        "        return self.dropout(X)"
      ],
      "metadata": {
        "id": "bhCxZ3nbL2p6"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}